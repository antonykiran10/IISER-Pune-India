# -*- coding: utf-8 -*-
"""custom_cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NuaZW_wz7ABGFmPy2B84aedcK-HceE2M

# Step 1 : Downaload the dataset and Unzip into two directories
"""

!wget https://ceb.nlm.nih.gov/proj/malaria/cell_images.zip

!unzip cell_images.zip

!apt install tree

# Show directory structure
!tree --dirsfirst --filelimit 10 /content/cell_images/

!pip install keras-tuner

#ensorflow and Keras Utils 
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import models, layers
from tensorflow.keras import Sequential 
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Activation, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import optimizers

# Numpy for Matrix Manipulatons 
import numpy as np

# Matplotlib and Seaborn for visualization
import matplotlib.pyplot as plt
import seaborn as sns ; sns.set()

# Sklearn for Model Utils
from sklearn.model_selection import train_test_split
print('Tensorflow Version:',tf.__version__)

"""# Step 2 : Data Preprocessing"""

data_directory = '/content/cell_images'
image_width = 64
image_height = 64
m = 27558

# Data Augmentation
datagen = ImageDataGenerator(shear_range=0.2,zoom_range=0.2,horizontal_flip=True,vertical_flip=True,rescale=1/255.0)

train_data_generator = datagen.flow_from_directory(directory='/content/cell_images', target_size=(image_width,image_height),
                                                   class_mode = 'binary', batch_size = m, shuffle=True, subset='training'
                            

# We use Data Generator to create a tensor dataset of all images form the given directories

X = train_data_generator[0][0]
Y = train_data_generator[0][1]

#  Here we separate the X and Y directories
print(X.shape)
print(Y.shape)

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
 
# We perform a 80/20 Train test Split of the Dataset

print(X_train.shape, y_train.shape) 
# use this data set for 80% training + 20% validation

print(X_test.shape,y_test.shape) 
# use this data for testing final score

"""# Step 3 : Building the Model"""

def build_model(X_train, y_train, X_val, y_val,epochs): 

    # Inspired from LENET 5 Architecture
    
    model = Sequential()

    model.add(Conv2D(6, (5,5), activation = 'relu', input_shape = X_train.shape[1:]))
    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
    model.add(BatchNormalization(axis = -1))
    model.add(Dropout(0.2))

    model.add(Conv2D(10, (5,5), activation = 'relu'))
    model.add(MaxPooling2D((2,2),strides=(1,1)))
    model.add(BatchNormalization(axis = -1))
    model.add(Dropout(0.2))

    model.add(Conv2D(16, (5,5), activation = 'relu'))
    model.add(MaxPooling2D((2,2),strides=(1,1)))
    model.add(BatchNormalization(axis = -1))
    model.add(Dropout(0.2))

    model.add(Flatten())

    model.add(Dense(120, activation = 'relu'))
    model.add(BatchNormalization(axis = -1))
    model.add(Dropout(0.5))

    model.add(Dense(84, activation = 'relu'))
    model.add(BatchNormalization(axis = -1))
    model.add(Dropout(0.5))

    model.add(Dense(10, activation = 'relu'))
    model.add(BatchNormalization(axis = -1))

    model.add(Dense(1, activation = 'sigmoid'))

    lr_schedule = keras.optimizers.schedules.ExponentialDecay(
                                initial_learning_rate=2e-3,
                                decay_steps=10000,
                                decay_rate=0.9)

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),
              loss="binary_crossentropy",
              metrics=["accuracy"])

    model.fit(X_train, y_train, batch_size=128, epochs=epochs, validation_data = (X_val, y_val))

    return model

"""# Step 4 : Training the Model"""

results = build_model(X_train, y_train,X_test, y_test, 40)

"""# Step 5 : Checking Model Accuracy and Entropy Loss"""

fig,axes = plt.subplots(nrows=2,ncols=1,figsize= (11,8))
#epochs = n_epochs
axes[0].plot(results.history.history['accuracy'],label='Training Accuracy')
axes[0].plot(results.history.history['val_accuracy'],label='Vaidation Accuracy')
axes[0].set_xlabel('No of Epochs',fontsize=14)
axes[0].set_ylabel('Accuracy (%)',fontsize=14)
axes[0].set_yticks(np.arange(0.5,1.01,0.05))
axes[0].set_title('Custom CNN Results Model Accuracy',fontsize=16)

axes[1].plot(results.history.history['loss'],label='Training loss')
axes[1].plot(results.history.history['val_loss'],label='Validation loss')
axes[1].set_xlabel('No of Epochs',fontsize=14)
axes[1].set_ylabel('Binary Cross Entropy',fontsize=14)
axes[1].set_title('Custom CNN Results Model Entropy',fontsize=16)
#axes[1].set_yticks(np.arange(0.0,0.9,0.1))


plt.tight_layout()
plt.show()

results.save('/content/drive/My Drive/DS-Python-notebooks/My-Projects/Malaria/custom_cnn_lenet_val_9539_oct_24_1100.h5')

